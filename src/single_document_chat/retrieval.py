import sys
import os
from pathlib import Path
from dotenv import load_dotenv
import streamlit as st

from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.vectorstores import FAISS
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from utils.model_loader import ModelLoader
from logger.custom_logger import CustomLogger
from exception.custom_exception_archive import DocumentPortalException
from prompt.prompt_library import PROMPT_REGISTRY
from model.models import *


class conversationalRAG:
    
    def __init__(self,session_id: str , retriever):
        try:
            self.log = CustomLogger().get_logger(__name__)
            self.session_id = session_id
            self.retriever = retriever
            self.llm = self._load_llm()
            
            self.contextualize_prompt = PROMPT_REGISTRY[promptType.CONTEXTULIZE_QUSTION.value]
            self.qa_prompt = PROMPT_REGISTRY[promptType.CONTEXT_QA.value]
            
            self.history_aware_retriever = create_history_aware_retriever(self.llm , self.retriever, self.contextualize_prompt)
            self.log.info("Created history aware retriever.", session_id=session_id)
            
            self.qa_chain = create_stuff_documents_chain(self.llm ,self.qa_prompt)
            self.log.info("Created QA chain.", session_id=session_id)
            
            self.rag_chain = create_retrieval_chain(self.history_aware_retriever,self.qa_chain)
            self.log.info("Created Retrieval Chain .", session_id=session_id)
            
            self.chain = RunnableWithMessageHistory(
                self.rag_chain,
                self._get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer"
                )
            
            self.log.info("RunnableWithMessageHistory :- ", session_id=session_id)
                                          
                                                    
        except Exception as e:
            self.log.error("Error initializing conversationalRAG:", error=str(e) , session_id=session_id)
            raise DocumentPortalException("Failed to initialize conversationalRAG", sys)

    def _load_llm(self):
        try:
            llm = ModelLoader().load_llm()
            self.log.info("LLM loaded successfully.", class_name=llm.__class__.__name__)
            return llm
            
        except Exception as e:
            self.log.error("Error loading LLM:", error=str(e))
            raise DocumentPortalException("Failed to load LLM", sys)
    
    def _get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        try:
            if "store" not in st.session_state:
                st.session_state.store= {}
                
            if session_id not in st.session_state.store:
                st.session_state.store[session_id] = ChatMessageHistory()
                self.log.info("Created new chat message history.", session_id=session_id)
            
            return st.session_state.store[session_id]
                
        except Exception as e:
            self.log.error("Error getting session history:", error=str(e))
            raise DocumentPortalException("Failed to get session history", sys)
        
    def load_retriever_from_faiss(self,index_path:str):
        try:
            embeddings = ModelLoader().load_embeddings()
            
            if not os.path.isdir(index_path):
                raise DocumentPortalException(f"FAISS index directory does not exist at path: {index_path}", sys)
            
            vector_store = FAISS.load_local(index_path, embeddings)
            self.log.info("FAISS vector store loaded successfully.", index_path=index_path)
            
            return vector_store.as_retriever(search_type = "similarity", search_kwargs={"k":5})
            
        except Exception as e:
            self.log.error("Error loading retriever:", error=str(e))
            raise DocumentPortalException("Failed to load retriever", sys)
        
    def invoke(self,user_input: str):
        try:
            self.log.info("Conversational RAG invoked successfully.", session_id=self.session_id)
            
            response = self.chain.invoke(
                {"input": user_input},
                config ={"configurable":{"session_id": self.session_id}}
                )
            
            answer = response.get("answer" ,"No answer generated.")
            if not answer:
                self.log.warning("No answer generated by the RAG chain.", session_id=self.session_id)
            
            self.log.info("Chain invoked successfully", session_id=self.session_id , user_input=user_input, answer_preview =answer[:50])
            return answer
            
            
            
        except Exception as e:
            self.log.error("Error invoking conversational RAG:", error=str(e))
            raise DocumentPortalException("Failed to invoke conversational RAG", sys)