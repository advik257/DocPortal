{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b34b7177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print('all ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69c6b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d6c0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f42f65cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nI need to introduce myself in a friendly and professional manner. When users ask about my name, I should clearly state my name, Qwen, and briefly mention my capabilities as a large language model developed by Alibaba Cloud's Tongyi Lab. I should keep the response concise while conveying my main functions and purpose to help users understand what I can do for them. At the same time, I should maintain a natural and approachable tone rather than being overly formal or technical. I need to make sure the response is in English since the user asked in English. I should also keep the response relatively short, not too lengthy, while still conveying the necessary information. The user is likely looking for a straightforward answer about my identity and capabilities, so I should focus on that without adding unnecessary details.\\n</think>\\n\\nHello! My name is Qwen. I'm a large language model developed by Alibaba Cloud's Tongyi Lab. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 14, 'total_tokens': 206, 'completion_time': 0.461342284, 'prompt_time': 0.000806438, 'queue_time': 0.081447472, 'total_time': 0.462148722}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4d8667d4-7693-49c3-9b4e-981b0033b8e4-0', usage_metadata={'input_tokens': 14, 'output_tokens': 192, 'total_tokens': 206})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, whats your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fced6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gemini Embedding Model\n",
    "embedding_model= GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\",google_api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b74c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace Embedding Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "Hugging_embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e374d4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.44772749e-02,  3.10231782e-02,  6.73497003e-03,  2.61089858e-02,\n",
       "       -3.93620245e-02, -1.60302445e-01,  6.69240132e-02, -6.44148979e-03,\n",
       "       -4.74504791e-02,  1.47588560e-02,  7.08752796e-02,  5.55276312e-02,\n",
       "        1.91933345e-02, -2.62513123e-02, -1.01095429e-02, -2.69404557e-02,\n",
       "        2.23074611e-02, -2.22266484e-02, -1.49692640e-01, -1.74930077e-02,\n",
       "        7.67625542e-03,  5.43522425e-02,  3.25439707e-03,  3.17258909e-02,\n",
       "       -8.46213847e-02, -2.94060130e-02,  5.15956134e-02,  4.81240600e-02,\n",
       "       -3.31482221e-03, -5.82791679e-02,  4.19692732e-02,  2.22106855e-02,\n",
       "        1.28188834e-01, -2.23389715e-02, -1.16563160e-02,  6.29283935e-02,\n",
       "       -3.28763351e-02, -9.12260413e-02, -3.11753470e-02,  5.26994914e-02,\n",
       "        4.70348299e-02, -8.42031166e-02, -3.00561991e-02, -2.07448304e-02,\n",
       "        9.51783545e-03, -3.72179062e-03,  7.34328525e-03,  3.93243879e-02,\n",
       "        9.32740644e-02, -3.78859648e-03, -5.27420677e-02, -5.80581613e-02,\n",
       "       -6.86436146e-03,  5.28319180e-03,  8.28929991e-02,  1.93627551e-02,\n",
       "        6.28448371e-03, -1.03307879e-02,  9.03237890e-03, -3.76836956e-02,\n",
       "       -4.52060774e-02,  2.40163058e-02, -6.94413716e-03,  1.34916306e-02,\n",
       "        1.00054942e-01, -7.16839135e-02, -2.16951203e-02,  3.16184051e-02,\n",
       "       -5.16346656e-02, -8.22477266e-02, -6.56933337e-02, -9.89533495e-03,\n",
       "        5.81637491e-03,  7.35545605e-02, -3.40503119e-02,  2.48861201e-02,\n",
       "        1.44880423e-02,  2.64573842e-02,  9.65672266e-03,  3.02172489e-02,\n",
       "        5.28039336e-02, -7.53598437e-02,  9.89714544e-03,  2.98368093e-02,\n",
       "        1.75555777e-02,  2.30919849e-02,  1.93380669e-03,  1.40025455e-03,\n",
       "       -4.71759588e-02, -1.11943157e-02, -1.14201441e-01, -1.98119245e-02,\n",
       "        4.02661897e-02,  2.19299062e-03, -7.97922090e-02, -2.53823176e-02,\n",
       "        9.44829956e-02, -2.89810430e-02, -1.45002529e-01,  2.30977446e-01,\n",
       "        2.77311876e-02,  3.21114697e-02,  3.10650580e-02,  4.28328477e-02,\n",
       "        6.42377734e-02,  3.21631618e-02, -4.87677054e-03,  5.56994639e-02,\n",
       "       -3.75323817e-02, -2.15055402e-02, -2.83426344e-02, -2.88469512e-02,\n",
       "        3.83530892e-02, -1.74686648e-02,  5.24853058e-02, -7.48760179e-02,\n",
       "       -3.12597640e-02,  2.18415651e-02, -3.98957059e-02, -8.58709123e-03,\n",
       "        2.69565769e-02, -4.84955385e-02,  1.14698829e-02,  2.96182036e-02,\n",
       "       -2.05721892e-02,  1.31038437e-02,  2.88335104e-02, -3.19419908e-33,\n",
       "        6.47821352e-02, -1.81301832e-02,  5.17899618e-02,  1.21982753e-01,\n",
       "        2.87801065e-02,  8.72195140e-03, -7.05211982e-02, -1.69072784e-02,\n",
       "        4.07397300e-02,  4.21161577e-02,  2.54472364e-02,  3.57462876e-02,\n",
       "       -4.91447151e-02,  2.12902040e-03, -1.55465826e-02,  5.07305451e-02,\n",
       "       -4.81853262e-02,  3.58806103e-02, -4.06704750e-03,  1.01724721e-01,\n",
       "       -5.59700206e-02, -1.06810490e-02,  1.12357857e-02,  9.06865373e-02,\n",
       "        4.23445133e-03,  3.51386555e-02, -9.70284734e-03, -9.38651785e-02,\n",
       "        9.28555429e-02,  8.00492708e-03, -7.70542538e-03, -5.20867445e-02,\n",
       "       -1.25879915e-02,  3.26693780e-03,  6.01350982e-03,  7.58155901e-03,\n",
       "        1.05171818e-02, -8.63455683e-02, -6.98788017e-02, -2.53389287e-03,\n",
       "       -9.09765884e-02,  4.68873307e-02,  5.20765409e-02,  7.19384430e-03,\n",
       "        1.09036230e-02, -5.22955880e-03,  1.39373112e-02,  2.19683498e-02,\n",
       "        3.42086628e-02,  6.02246821e-02,  1.16654701e-04,  1.47319762e-02,\n",
       "       -7.00892657e-02,  2.84990482e-02, -2.76017208e-02,  1.07684452e-02,\n",
       "        3.48309614e-02, -2.24878713e-02,  9.76901781e-03,  7.72278532e-02,\n",
       "        2.15883143e-02,  1.14956208e-01, -6.80011734e-02,  2.37609856e-02,\n",
       "       -1.59839429e-02, -1.78269912e-02,  6.43949583e-02,  3.20257396e-02,\n",
       "        5.02702519e-02, -5.91377076e-03, -3.37080508e-02,  1.78402569e-02,\n",
       "        1.65733173e-02,  6.32965788e-02,  3.46772186e-02,  4.64734882e-02,\n",
       "        9.79061052e-02, -6.63550291e-03,  2.52071284e-02, -7.79882446e-02,\n",
       "        1.69264767e-02, -9.45797365e-04,  2.24719215e-02, -3.82531919e-02,\n",
       "        9.57047418e-02, -5.35080303e-03,  1.04691107e-02, -1.15240552e-01,\n",
       "       -1.32625215e-02, -1.07094552e-02, -8.31172541e-02,  7.32735395e-02,\n",
       "        4.93922569e-02, -8.99432227e-03, -9.58455205e-02,  3.36614856e-33,\n",
       "        1.24931842e-01,  1.93497203e-02, -5.82257174e-02, -3.59882638e-02,\n",
       "       -5.07467650e-02, -4.56623845e-02, -8.26033652e-02,  1.48194805e-01,\n",
       "       -8.84211883e-02,  6.02744371e-02,  5.10301590e-02,  1.03031471e-02,\n",
       "        1.41214222e-01,  3.08138449e-02,  6.10331595e-02, -5.28512709e-02,\n",
       "        1.36648968e-01,  9.18989722e-03, -1.73251890e-02, -1.28485551e-02,\n",
       "       -7.99528230e-03, -5.09800985e-02, -5.23506477e-02,  7.59301288e-03,\n",
       "       -1.51663078e-02,  1.69603098e-02,  2.12705210e-02,  2.05581076e-02,\n",
       "       -1.20028138e-01,  1.44618340e-02,  2.67599188e-02,  2.53306963e-02,\n",
       "       -4.27546352e-02,  6.76838728e-03, -1.44585846e-02,  4.52619568e-02,\n",
       "       -9.14764851e-02, -1.94391459e-02, -1.78334676e-02, -5.49101830e-02,\n",
       "       -5.26411124e-02, -1.04590487e-02, -5.20160869e-02,  2.08919551e-02,\n",
       "       -7.99703673e-02, -1.21113406e-02, -5.77314273e-02,  2.31782347e-02,\n",
       "       -8.03173240e-03, -2.59893034e-02, -7.99567103e-02, -2.07288321e-02,\n",
       "        4.88176979e-02, -2.03891378e-02, -4.91765775e-02,  1.41596226e-02,\n",
       "       -6.36220202e-02, -7.80739309e-03,  1.64315570e-02, -2.56824791e-02,\n",
       "        1.33810407e-02,  2.62487419e-02,  9.97841358e-03,  6.32288679e-02,\n",
       "        2.67220126e-03, -6.58276724e-03,  1.66318826e-02,  3.23664658e-02,\n",
       "        3.79424579e-02, -3.63760702e-02, -6.91093039e-03,  1.59692805e-04,\n",
       "       -1.63358089e-03, -2.72782128e-02, -2.80380733e-02,  4.96814176e-02,\n",
       "       -2.88671739e-02, -2.41806894e-03,  1.47749083e-02,  9.76453442e-03,\n",
       "        5.79763809e-03,  1.34861609e-02,  5.56789571e-03,  3.72271053e-02,\n",
       "        7.23252725e-03,  4.01562639e-02,  8.15032646e-02,  7.19916746e-02,\n",
       "       -1.30561264e-02, -4.28820476e-02, -1.10112382e-02,  4.89782030e-03,\n",
       "       -9.22973081e-03,  3.51915061e-02, -5.10350242e-02, -1.57143756e-08,\n",
       "       -8.86244103e-02,  2.39092596e-02, -1.62387639e-02,  3.17005105e-02,\n",
       "        2.72842478e-02,  5.24688289e-02, -4.70709577e-02, -5.88474460e-02,\n",
       "       -6.32082298e-02,  4.08884957e-02,  4.98280041e-02,  1.06551714e-01,\n",
       "       -7.45023042e-02, -1.24954218e-02,  1.83707122e-02,  3.94741260e-02,\n",
       "       -2.47978866e-02,  1.45162623e-02, -3.70692164e-02,  2.00157277e-02,\n",
       "       -4.85817036e-05,  9.86657664e-03,  2.48387530e-02, -5.24581410e-02,\n",
       "        2.93141790e-02, -8.71919096e-02, -1.44996876e-02,  2.60190777e-02,\n",
       "       -1.87463667e-02, -7.62051269e-02,  3.50433327e-02,  1.03639498e-01,\n",
       "       -2.80505102e-02,  1.27181830e-02, -7.63254911e-02, -1.86523218e-02,\n",
       "        2.49767285e-02,  8.14453512e-02,  6.87588379e-02, -6.40566498e-02,\n",
       "       -8.38938579e-02,  6.13623187e-02, -3.35455649e-02, -1.06153369e-01,\n",
       "       -4.00805883e-02,  3.25302258e-02,  7.66248330e-02, -7.30162114e-02,\n",
       "        3.37552658e-04, -4.08716463e-02, -7.57884756e-02,  2.75276657e-02,\n",
       "        7.46254325e-02,  1.77172683e-02,  9.12184641e-02,  1.10220164e-01,\n",
       "        5.69773139e-04,  5.14633618e-02, -1.45513108e-02,  3.32320370e-02,\n",
       "        2.37922408e-02, -2.28898060e-02,  3.89375389e-02,  3.02068442e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding_model.embed_query(\"Hello world\")\n",
    "Hugging_embedding_model.encode(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f973b",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d56cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "005b2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6abadf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b8bc1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf66a48",
   "metadata": {},
   "source": [
    "# this is a experimental thing there is no deterministic way to split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3df8185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100 , length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ffe1b8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)\n",
    "# 662 Documents available after chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21e4d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[0].metadata\n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a908243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aabc6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hug_embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "976d42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedlen=hug_embedding_model.embed_documents(docs[0].page_content)\n",
    "#len(embedlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09394882",
   "metadata": {},
   "source": [
    "In Memory storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d4ff9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, hug_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ea953",
   "metadata": {},
   "source": [
    "## Retrieval process - from the vector database , going to fetch or retrieve most relevant documents/ Ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ca47e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ae06a9b0-3328-4710-a594-a0586a340258', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='Llama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks\\nwe selected are representative of our understanding thatLlama 2-Chatimproves on critical aspects of LLM'),\n",
       " Document(id='91655f1a-8186-4008-bf66-39998f29e6ad', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='• Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks\\net al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3,Llama 2models outperformLlama 1models. In particular,Llama 270B improves the\\nresultsonMMLUandBBHby ≈5and ≈8points, respectively, comparedtoLlama 165B.Llama 27Band30B'),\n",
       " Document(id='6aaae55a-63c5-4a73-8087-5e560614e210', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,\\nLlama 2-7B demonstrates a 21.37% increase in truthfulness and informativeness and a 7.61% decrease in\\ntoxicity. We also observe an increase in toxicity in the pretrained 13B and 70BLlama 2, which may result\\nfrom larger pretraining data or a different dataset mix. Some have postulated the existence of a relationship'),\n",
       " Document(id='9bf1f5c6-67e3-4008-9bc2-f2213fbb214a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='in Table 4,Llama 270B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance betweenLlama 270B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_results = vector_store.similarity_search(\"llama2 finetuning benchmark experiments?\")\n",
    "relevant_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5ece1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,\\nLlama 2-7B demonstrates a 21.37% increase in truthfulness and informativeness and a 7.61% decrease in\\ntoxicity. We also observe an increase in toxicity in the pretrained 13B and 70BLlama 2, which may result\\nfrom larger pretraining data or a different dataset mix. Some have postulated the existence of a relationship'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_results[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cae13319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ae06a9b0-3328-4710-a594-a0586a340258', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='Llama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks\\nwe selected are representative of our understanding thatLlama 2-Chatimproves on critical aspects of LLM'),\n",
       " Document(id='91655f1a-8186-4008-bf66-39998f29e6ad', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='• Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks\\net al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3,Llama 2models outperformLlama 1models. In particular,Llama 270B improves the\\nresultsonMMLUandBBHby ≈5and ≈8points, respectively, comparedtoLlama 165B.Llama 27Band30B')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default k=4\n",
    "relevant_results = vector_store.similarity_search(\"llama2 finetuning benchmark experiments?\" ,k=2)\n",
    "relevant_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec626aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f6fcf941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ae06a9b0-3328-4710-a594-a0586a340258', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='Llama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks\\nwe selected are representative of our understanding thatLlama 2-Chatimproves on critical aspects of LLM'),\n",
       " Document(id='91655f1a-8186-4008-bf66-39998f29e6ad', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='• Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks\\net al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3–5 shot) (Zhong\\net al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.\\nAs shown in Table 3,Llama 2models outperformLlama 1models. In particular,Llama 270B improves the\\nresultsonMMLUandBBHby ≈5and ≈8points, respectively, comparedtoLlama 165B.Llama 27Band30B'),\n",
       " Document(id='6aaae55a-63c5-4a73-8087-5e560614e210', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 21, 'page_label': '22'}, page_content='descriptions of the benchmarks and metrics can be found in Appendix A.4.7. When compared toLlama 1-7B,\\nLlama 2-7B demonstrates a 21.37% increase in truthfulness and informativeness and a 7.61% decrease in\\ntoxicity. We also observe an increase in toxicity in the pretrained 13B and 70BLlama 2, which may result\\nfrom larger pretraining data or a different dataset mix. Some have postulated the existence of a relationship'),\n",
       " Document(id='9bf1f5c6-67e3-4008-9bc2-f2213fbb214a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'e:\\\\LLMProjects\\\\documentportal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='in Table 4,Llama 270B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance betweenLlama 270B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad916686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question - user query\n",
    "# context - relevant documents retrieved from vector database\n",
    "# response - answer generated by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6804d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Answer the question based on the context below.\n",
    "if the context is not relevant, say 'I don't know'\n",
    "\n",
    "context: {context}\n",
    "question: {question}\n",
    "answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25b74786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Answer the question based on the context below.\\nif the context is not relevant, say 'I don't know'\\n\\ncontext: {context}\\nquestion: {question}\\nanswer:\\n\")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6896346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8f61c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fa090253",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "#rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bdc96e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s tackle this query. The user is asking about \"llama2 finetuning benchmark experiments.\" First, I need to check the provided context to see if it mentions anything about fine-tuning benchmarks for Llama 2.\\n\\nLooking at the context, there\\'s a table (Table 44) showing evaluation results for Llama 2-Chat across different model sizes on the TruthfulQA benchmark. The context also discusses limitations of benchmarks and mentions other benchmarks like MMLU, BBH, and AGI Eval. However, the main focus is on comparing Llama 2 models with previous versions and other models like GPT-3.5, GPT-4, PaLM, and PaLM-2-L. \\n\\nThe text talks about improvements in truthfulness, informativeness, and toxicity, but there\\'s no specific mention of fine-tuning experiments. The benchmarks discussed are for the base models, not fine-tuned versions. The section about data contamination is in Appendix A.6, but that\\'s not detailed here. Since the user is asking about fine-tuning benchmarks and the context doesn\\'t reference any experiments related to that, I should conclude that the information isn\\'t present here. Therefore, the correct response is to state that the context doesn\\'t provide relevant information.\\n</think>\\n\\nI don\\'t know'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44cf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1dddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3317c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2149d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56096c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976d740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
